{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pprint\n",
    "from nltk.corpus import brown\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_sents = brown.tagged_sents(categories='news')\n",
    "size = int(len(tagged_sents) * 0.1)\n",
    "train_sents, test_sents = tagged_sents[size:], tagged_sents[:size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_features(sentence, i, history):\n",
    "    features = {\"suffix(1)\": sentence[i][-1:],\n",
    "                \"suffix(2)\": sentence[i][-2:],\n",
    "                \"suffix(3)\": sentence[i][-3:]}\n",
    "    if i == 0:\n",
    "        features[\"prev-word\"] = \"<START>\"\n",
    "    else:\n",
    "        features[\"prev-word\"] = sentence[i-1]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConsecutivePosTagger(nltk.TaggerI):\n",
    "    \n",
    "    def __init__(self, train_sents, features=pos_features):\n",
    "        self.features = features\n",
    "        train_set = []\n",
    "        for tagged_sent in train_sents:\n",
    "            untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "            history = []\n",
    "            for i, (word, tag) in enumerate(tagged_sent):\n",
    "                featureset = features(untagged_sent, i, history)\n",
    "                train_set.append( (featureset, tag) )\n",
    "                history.append(tag)\n",
    "        self.classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "        \n",
    "    def tag(self, sentence):\n",
    "        history = []\n",
    "        for i, word in enumerate(sentence):\n",
    "            featureset = self.features(sentence, i, history)\n",
    "            tag = self.classifier.classify(featureset)\n",
    "            history.append(tag)\n",
    "        return zip(sentence, history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7915\n"
     ]
    }
   ],
   "source": [
    "tagger = ConsecutivePosTagger(train_sents)\n",
    "print(round(tagger.evaluate(test_sents), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8314\n"
     ]
    }
   ],
   "source": [
    "def originize(tagged_sents):\n",
    "    \"\"\"Change tags to original Brown tags in tagged_sents\"\"\"\n",
    "    return [ [(word, tag.split('-')[0]) for (word,tag) in sent]\n",
    "            for sent in tagged_sents]\n",
    "\n",
    "orig_train_sents = originize(train_sents)\n",
    "orig_test_sents = originize(test_sents)\n",
    "\n",
    "orig_tagger_1 = ConsecutivePosTagger(orig_train_sents)\n",
    "print(round(orig_tagger_1.evaluate(orig_test_sents), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I found out that len(tagged_sents) is 4623, and 10% of that is = 462.3, so I rounded it down to 462\n",
    "# 10% for final testing\n",
    "news_test = tagged_sents[:462]\n",
    "# 10% for development testing\n",
    "news_dev_test = tagged_sents[462:924]\n",
    "# 80% for training\n",
    "news_train = tagged_sents[924:]\n",
    "# print(len(news_test), len(news_dev_test),len(news_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8569\n"
     ]
    }
   ],
   "source": [
    "train_sents = originize(news_train)\n",
    "test_sents = originize(news_dev_test)\n",
    "baseline = ConsecutivePosTagger(orig_train_sents)\n",
    "print(round(baseline.evaluate(test_sents), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, the baseline beats the NLTK tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "\n",
    "class ScikitConsecutivePosTagger(nltk.TaggerI):\n",
    "    def __init__(self, train_sents,\n",
    "                 features=pos_features, clf = BernoulliNB(alpha=0.5)):\n",
    "        # Using pos_features as default.\n",
    "        # Using BernoulliNB() (with alpha/lidstone 0.5)\n",
    "        self.features = features\n",
    "        train_features = []\n",
    "        train_labels = []\n",
    "        for tagged_sent in train_sents:\n",
    "            history = []\n",
    "            untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "            \n",
    "            for i, (word, tag) in enumerate(tagged_sent):\n",
    "                featureset = features(untagged_sent, i, history)\n",
    "                train_features.append(featureset)\n",
    "                train_labels.append(tag)\n",
    "                history.append(tag)\n",
    "        v = DictVectorizer()\n",
    "        X_train = v.fit_transform(train_features)\n",
    "        y_train = np.array(train_labels)\n",
    "        clf.fit(X_train, y_train)\n",
    "        self.classifier = clf\n",
    "        self.dict = v\n",
    "\n",
    "    def tag(self, sentence):\n",
    "        test_features = []\n",
    "        history = []\n",
    "        for i, word in enumerate(sentence):\n",
    "            featureset = self.features(sentence, i, history)\n",
    "            test_features.append(featureset)\n",
    "        X_test = self.dict.transform(test_features)\n",
    "        tags = self.classifier.predict(X_test)\n",
    "        return zip(sentence, tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7654\n"
     ]
    }
   ],
   "source": [
    "orig_tagger_2 = ScikitConsecutivePosTagger(train_sents)\n",
    "print(round(orig_tagger_2.evaluate(test_sents), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, it did not yield the same result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 1, score: 0.6646\n",
      "Alpha: 0.5, score: 0.7654\n",
      "Alpha: 0.1, score: 0.8309\n",
      "Alpha: 0.01, score: 0.8276\n",
      "Alpha: 0.001, score: 0.8237\n",
      "Alpha: 0.0001, score: 0.8269\n"
     ]
    }
   ],
   "source": [
    "alpha = [1, 0.5, 0.1, 0.01, 0.001, 0.0001]\n",
    "\n",
    "for a in alpha:\n",
    "    orig_tagger = ScikitConsecutivePosTagger(train_sents, clf=BernoulliNB(alpha=a))\n",
    "    print(\"Alpha: {}, score: {}\".format(a,round(orig_tagger.evaluate(test_sents), 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best result with the new tagger was with alpha: 0.1 with a score of: 0.8309\n",
    "\n",
    "This was a little better than NLTK, which was at 0.8249\n",
    "\n",
    "### Exercise 2.2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_pos_features(sentence, i, history):\n",
    "    features = {\"suffix(1)\": sentence[i][-1:],\n",
    "                \"suffix(2)\": sentence[i][-2:],\n",
    "                \"suffix(3)\": sentence[i][-3:]}\n",
    "    if i == 0:\n",
    "        features[\"prev-word\"] = \"<START>\"\n",
    "    else:\n",
    "        features[\"prev-word\"] = sentence[i-1]\n",
    "    features[\"this-word\"] = sentence[i]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 1, score: 0.6553\n",
      "Alpha: 0.5, score: 0.7857\n",
      "Alpha: 0.1, score: 0.8824\n",
      "Alpha: 0.01, score: 0.8912\n",
      "Alpha: 0.001, score: 0.8971\n",
      "Alpha: 0.0001, score: 0.9024\n"
     ]
    }
   ],
   "source": [
    "for a in alpha:\n",
    "    orig_tagger = ScikitConsecutivePosTagger(train_sents, features= new_pos_features, clf=BernoulliNB(alpha=a))\n",
    "    print(\"Alpha: {}, score: {}\".format(a,round(orig_tagger.evaluate(test_sents), 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extended the feature selector, so it should get a better result.\n",
    "Yes, the extended feature selector beat the baseline.\n",
    "\n",
    "The best result was with alpha: 0.0001 with a score of 0.9024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScikitConsecutivePosTagger(nltk.TaggerI):\n",
    "    def __init__(self, train_sents,\n",
    "                 features=pos_features, clf = LogisticRegression(solver='lbfgs', C=1.0)): # Default C to 1, since it already does this\n",
    "        # Using pos_features as default.\n",
    "        # Using BernoulliNB() (with alpha/lidstone 0.5)\n",
    "        self.features = features\n",
    "        train_features = []\n",
    "        train_labels = []\n",
    "        for tagged_sent in train_sents:\n",
    "            history = []\n",
    "            untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "            \n",
    "            for i, (word, tag) in enumerate(tagged_sent):\n",
    "                featureset = features(untagged_sent, i, history)\n",
    "                train_features.append(featureset)\n",
    "                train_labels.append(tag)\n",
    "                history.append(tag)\n",
    "        v = DictVectorizer()\n",
    "        X_train = v.fit_transform(train_features)\n",
    "        y_train = np.array(train_labels)\n",
    "        clf.fit(X_train, y_train)\n",
    "        self.classifier = clf\n",
    "        self.dict = v\n",
    "\n",
    "    def tag(self, sentence):\n",
    "        test_features = []\n",
    "        history = []\n",
    "        for i, word in enumerate(sentence):\n",
    "            featureset = self.features(sentence, i, history)\n",
    "            test_features.append(featureset)\n",
    "        X_test = self.dict.transform(test_features)\n",
    "        tags = self.classifier.predict(X_test)\n",
    "        return zip(sentence, tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.9263\n"
     ]
    }
   ],
   "source": [
    "log_tagger = ScikitConsecutivePosTagger(train_sents, features= new_pos_features, clf=LogisticRegression(solver='lbfgs'))\n",
    "print(\"score: {}\".format(round(log_tagger.evaluate(test_sents), 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, this worked better than Bernoulli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C: 0.01, score: 0.769\n",
      "C: 0.1, score: 0.8739\n",
      "C: 1.0, score: 0.926\n",
      "C: 10.0, score: 0.9356\n",
      "C: 100.0, score: 0.9345\n"
     ]
    }
   ],
   "source": [
    "c = [0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "\n",
    "for a in c:\n",
    "    orig_tagger = ScikitConsecutivePosTagger(train_sents, features= new_pos_features, clf=LogisticRegression(C=a))\n",
    "    print(\"C: {}, score: {}\".format(a,round(orig_tagger.evaluate(test_sents), 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C = 10.0 yields the best result with 0.9356, and is better than Naive Bayes\n",
    "\n",
    "### Exercise 2.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_new_pos_features(sentence, i, history):\n",
    "    features = {\"suffix(1)\": sentence[i][-1:],\n",
    "                \"suffix(2)\": sentence[i][-2:],\n",
    "                \"suffix(3)\": sentence[i][-3:]}\n",
    "    if i == 0:\n",
    "        features[\"prev-word\"] = \"<START>\"\n",
    "    else:\n",
    "        features[\"prev-word\"] = sentence[i-1]\n",
    "    features[\"this-word\"] = sentence[i]\n",
    "    \n",
    "    if i+1 >= len(sentence):\n",
    "        features[\"next-word\"] = \"<END>\"\n",
    "    else:\n",
    "        features[\"next-word\"] = sentence[i+1]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score with next-word feature : 0.948\n"
     ]
    }
   ],
   "source": [
    "orig_tagger = ScikitConsecutivePosTagger(train_sents, features=new_new_pos_features, clf=LogisticRegression(C=10.0))\n",
    "print(\"score with next-word feature : {}\".format(round(orig_tagger.evaluate(test_sents), 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_new_new_pos_features(sentence, i, history):\n",
    "    features = {\"suffix(1)\": sentence[i][-1:],\n",
    "                \"suffix(2)\": sentence[i][-2:],\n",
    "                \"suffix(3)\": sentence[i][-3:]}\n",
    "    if i == 0:\n",
    "        features[\"prev-word\"] = \"<START>\"\n",
    "    else:\n",
    "        features[\"prev-word\"] = sentence[i-1]\n",
    "    features[\"this-word\"] = sentence[i]\n",
    "    \n",
    "    if i+1 >= len(sentence):\n",
    "        features[\"next-word\"] = \"<END>\"\n",
    "    else:\n",
    "        features[\"next-word\"] = sentence[i+1]\n",
    "    \n",
    "    features[\"isnumeric\"] = sentence[i].isnumeric()\n",
    "    features[\"upperFirstLetter\"] = sentence[i][0].isupper()\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score with isnumeric and upperFirstLetter feature : 0.9535\n"
     ]
    }
   ],
   "source": [
    "orig_tagger = ScikitConsecutivePosTagger(train_sents, features=new_new_new_pos_features, clf=LogisticRegression(C=10.0, solver='lbfgs'))\n",
    "print(\"score with isnumeric and upperFirstLetter feature : {}\".format(round(orig_tagger.evaluate(test_sents), 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score with isnumeric and upperFirstLetter feature : 0.8938\n"
     ]
    }
   ],
   "source": [
    "print(\"score with isnumeric and upperFirstLetter feature : {}\".format(round(orig_tagger.evaluate(news_test), 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base' : 0.9414\n"
     ]
    }
   ],
   "source": [
    "# brown.categories()\n",
    "cat = ['belles_lettres', 'editorial', 'fiction', 'government', 'humor', 'learned',\n",
    "       'lore', 'mystery', 'religion', 'reviews', 'romance', 'science_fiction']\n",
    "all_tagged_sents = brown.tagged_sents(categories=cat)\n",
    "size = int(len(all_tagged_sents) * 0.1)\n",
    "rest_train, rest_dev_test, rest_test = all_tagged_sents[size:-size], all_tagged_sents[:size], all_tagged_sents[-size:]\n",
    "rest_train = originize(rest_train)\n",
    "rest_dev_test = originize(rest_dev_test)\n",
    "rest_test = originize(rest_test)\n",
    "\n",
    "train = rest_train+news_train\n",
    "test = rest_test + news_test\n",
    "\n",
    "orig_tagger = ScikitConsecutivePosTagger(news_train, features=new_new_new_pos_features, clf=LogisticRegression(C=10.0, solver='lbfgs'))\n",
    "print(\"Base' : {}\".format(round(orig_tagger.evaluate(news_test), 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.5.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score with all categories except 'hobbies' and 'adventure' : 0.9556\n"
     ]
    }
   ],
   "source": [
    "orig_tagger = ScikitConsecutivePosTagger(train, features=new_new_new_pos_features, clf=LogisticRegression(C=10.0, solver='lbfgs'))\n",
    "print(\"score with all categories except 'hobbies' and 'adventure' : {}\".format(round(orig_tagger.evaluate(test), 4)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.5.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score with hobbies : 0.9501\n",
      "score with adventure : 0.9594\n"
     ]
    }
   ],
   "source": [
    "hobbies_sents = brown.tagged_sents(categories='hobbies')\n",
    "adventure_sents = brown.tagged_sents(categories='adventure')\n",
    "\n",
    "hobbies_sents = originize(hobbies_sents)\n",
    "adventure_sents = originize(adventure_sents)\n",
    "\n",
    "print(\"score with hobbies : {}\".format(round(orig_tagger.evaluate(hobbies_sents), 4)))\n",
    "print(\"score with adventure : {}\".format(round(orig_tagger.evaluate(adventure_sents), 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results were within margin the same, but there are of course some words used in the different categories that did not occur that often in other categories.\n",
    "\n",
    "The fact that they are so close (adventure beating the 'all categories but hobbies and adventure'), I think maybe I did something wrong.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.6.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8924119999992399 seconds\n",
      "HMM-tagger with news_train: 0.8597\n"
     ]
    }
   ],
   "source": [
    "# Start timer\n",
    "start_time = time.clock()\n",
    "news_hmm_tagger = nltk.HiddenMarkovModelTagger.train(news_train)\n",
    "print(time.clock() - start_time, \"seconds\")\n",
    "print(\"HMM-tagger with news_train: {}\".format(round(news_hmm_tagger.evaluate(news_test), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMM-tagger with news_train: 0.9225\n",
      "224.80315399999927 seconds\n"
     ]
    }
   ],
   "source": [
    "# Start timer\n",
    "start_time = time.clock()\n",
    "news_hmm_tagger = nltk.HiddenMarkovModelTagger.train(train)\n",
    "print(\"HMM-tagger with news_train: {}\".format(round(news_hmm_tagger.evaluate(test), 4)))\n",
    "print(time.clock() - start_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HMM tagger was a little worse in this test than my tagger, but it ran pretty fast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.6.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron-tagger with news_train: 0.9225\n",
      "694.7592920000006 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.clock()\n",
    "per_tagger = nltk.PerceptronTagger(load=False)\n",
    "per_tagger.train(train)\n",
    "print(\"Perceptron-tagger with news_train: {}\".format(round(news_hmm_tagger.evaluate(test), 4)))\n",
    "print(time.clock() - start_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This took way longer, and was not as good as mine was. (Not saying that I made a better overall tagger, and that I did not do anything wrong)\n",
    "\n",
    "The time it took compared to the HMM-tagger was about three times longer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
