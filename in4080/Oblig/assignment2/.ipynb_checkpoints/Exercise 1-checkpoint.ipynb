{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise 1.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I just called it more (MOvie REviews) to write less\n",
    "from nltk.corpus import movie_reviews as more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_movie_docs = [\n",
    "    (more.raw(fileid), category) for category in more.categories()\n",
    "    for fileid in more.fileids(category)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(15)\n",
    "random.shuffle(raw_movie_docs)\n",
    "# The first 200 random documents will be for final testing\n",
    "movie_test = raw_movie_docs[:200]\n",
    "# The last 1800 documents (we know there are 2000 documents) will be for training\n",
    "movie_dev = raw_movie_docs[200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1600 docs for development test data\n",
    "train_data = movie_dev[200:]\n",
    "# 200 docs for train-data\n",
    "dev_test_data = movie_dev[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lists of the texts and targets (positive og negative) from train_data\n",
    "train_texts = []\n",
    "train_target = []\n",
    "for review in train_data:\n",
    "    text, sentiment = review\n",
    "    train_texts.append(text)\n",
    "    train_target.append(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lists of the texts and targets (positive og negative) from dev_test_data\n",
    "dev_test_texts = []\n",
    "dev_test_target = []\n",
    "for review in dev_test_data:\n",
    "    text, sentiment = review\n",
    "    dev_test_texts.append(text)\n",
    "    dev_test_target.append(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Considers the whole set of training data, to determine which features to extract\n",
    "v = CountVectorizer()\n",
    "v.fit(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from the training data and the test data\n",
    "train_vectors = v.transform(train_texts)\n",
    "dev_test_vectors = v.transform(dev_test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train classifier\n",
    "clf = MultinomialNB()\n",
    "clf.fit(train_vectors, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict one \n",
    "#print(dev_test_texts[14])\n",
    "#clf.predict(dev_test_vectors[14])\n",
    "# Predict all\n",
    "#clf.predict(dev_test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the accuracy of the multinomial naive Bayes text classifier\n",
    "clf.score(dev_test_vectors, dev_test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_nb_exp(train_data, test_data):\n",
    "    def separate(data):\n",
    "        texts = []\n",
    "        targets = []\n",
    "        for review in data:\n",
    "            text, sentiment = review\n",
    "            texts.append(text)\n",
    "            targets.append(sentiment)\n",
    "        return (texts, targets)\n",
    "    train_texts, train_target = separate(train_data)\n",
    "    test_texts, test_target = separate(test_data)\n",
    "    \n",
    "    v = CountVectorizer()\n",
    "    v.fit(train_texts)\n",
    "    \n",
    "    train_vectors = v.transform(train_texts)\n",
    "    test_vectors = v.transform(test_texts)\n",
    "    \n",
    "    \n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(train_vectors, train_target)\n",
    "    \n",
    "    prediction = clf.predict(test_vectors)\n",
    "    accuracy = clf.score(test_vectors, test_target)\n",
    "    \n",
    "    return (accuracy, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.81\n"
     ]
    }
   ],
   "source": [
    "(acc, pred) = multi_nb_exp(train_data, dev_test_data)\n",
    "print(\"acc:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def n_fold_kwargs(experiment, dev_data, folds=10, **kwargs):\n",
    "    # Assume that the data can be devided by folds into equal folds.\n",
    "    size = len(dev_data)/folds\n",
    "    acc_sum = 0\n",
    "    sum_squared_diff = 0\n",
    "    accuracies = []\n",
    "    data_folds = []\n",
    "    for x in range(folds):\n",
    "        data_folds.append(dev_data[(x*int(size)):((x+1)*int(size))])\n",
    "    \n",
    "    for run in range(folds):\n",
    "        train_tmp = [k for k in data_folds if k is not data_folds[run]]\n",
    "        test = data_folds[run]\n",
    "        train = []\n",
    "        for x in train_tmp:\n",
    "            train = train + x\n",
    "        (acc, pred) = experiment(train, test)\n",
    "        accuracies.append(acc)\n",
    "        acc_sum = acc_sum + acc\n",
    "    mean = acc_sum/folds\n",
    "    \n",
    "    for x in accuracies:\n",
    "        sum_squared_diff = sum_squared_diff + ((x-mean)*(x-mean))\n",
    "    standard_deviation = math.sqrt(sum_squared_diff/(folds-1))\n",
    "    \n",
    "    return (accuracies, mean, standard_deviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: [0.81, 0.815, 0.81, 0.75, 0.815, 0.815, 0.81, 0.82, 0.82]\n",
      "mean: 0.8072222222222223\n",
      "Standard: 0.021810420547170657\n"
     ]
    }
   ],
   "source": [
    "(acc, mean, standard) = n_fold_kwargs(multi_nb_exp, movie_dev, folds=9)\n",
    "print(\"acc:\", acc)\n",
    "print(\"mean:\", mean)\n",
    "print(\"Standard:\", standard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note_:\n",
    "If I understand this exercise correctly, I'm supposed to use the \\*\\*kwargs to pass in ngram_range= and binary=.\n",
    "I was not able to do this. Could not figure out how. That is why I made a new function that takes them in directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_nb_experiment(train_data, test_data, ngram_range, binary):\n",
    "    def separate(data):\n",
    "        texts = []\n",
    "        targets = []\n",
    "        for review in data:\n",
    "            text, sentiment = review\n",
    "            texts.append(text)\n",
    "            targets.append(sentiment)\n",
    "        return (texts, targets)\n",
    "    train_texts, train_target = separate(train_data)\n",
    "    test_texts, test_target = separate(test_data)\n",
    "    \n",
    "    v = CountVectorizer(ngram_range=ngram_range, binary=binary)\n",
    "    v.fit(train_texts)\n",
    "    \n",
    "    train_vectors = v.transform(train_texts)\n",
    "    test_vectors = v.transform(test_texts)\n",
    "    \n",
    "    \n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(train_vectors, train_target)\n",
    "    \n",
    "    prediction = clf.predict(test_vectors)\n",
    "    accuracy = clf.score(test_vectors, test_target)\n",
    "    \n",
    "    return (accuracy, prediction)\n",
    "\n",
    "def n_folds(experiment, dev_data, ngram_range, binary, folds=10):\n",
    "    # Assume that the data can be devided by folds into equal folds.\n",
    "    size = len(dev_data)/folds\n",
    "    acc_sum = 0\n",
    "    sum_squared_diff = 0\n",
    "    accuracies = []\n",
    "    data_folds = []\n",
    "    for x in range(folds):\n",
    "        data_folds.append(dev_data[(x*int(size)):((x+1)*int(size))])\n",
    "    \n",
    "    for run in range(folds):\n",
    "        train_tmp = [k for k in data_folds if k is not data_folds[run]]\n",
    "        test = data_folds[run]\n",
    "        train = []\n",
    "        for x in train_tmp:\n",
    "            train = train + x\n",
    "        (acc, pred) = experiment(train, test, ngram_range, binary)\n",
    "        accuracies.append(acc)\n",
    "        acc_sum = acc_sum + acc\n",
    "    mean = acc_sum/folds\n",
    "    \n",
    "    for x in accuracies:\n",
    "        sum_squared_diff = sum_squared_diff + ((x-mean)*(x-mean))\n",
    "    standard_deviation = math.sqrt(sum_squared_diff/(folds-1))\n",
    "    \n",
    "    return (accuracies, mean, standard_deviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "highest: 0.88\n"
     ]
    }
   ],
   "source": [
    "highest = 0\n",
    "\n",
    "(acc1, mean1, standard1) = n_folds(multi_nb_experiment, movie_dev, folds=9, ngram_range=[1,1], binary=True)\n",
    "if max(acc1) > highest:\n",
    "    highest = max(acc1)\n",
    "\n",
    "(acc2, mean2, standard2) = n_folds(multi_nb_experiment, movie_dev, folds=9, ngram_range=[1,2], binary=True)\n",
    "if max(acc2) > highest:\n",
    "    highest = max(acc2)\n",
    "\n",
    "(acc3, mean3, standard3) = n_folds(multi_nb_experiment, movie_dev, folds=9, ngram_range=[1,3], binary=True)\n",
    "if max(acc3) > highest:\n",
    "    highest = max(acc3)\n",
    "\n",
    "(acc4, mean4, standard4) = n_folds(multi_nb_experiment, movie_dev, folds=9, ngram_range=[1,1], binary=False)\n",
    "if max(acc4) > highest:\n",
    "    highest = max(acc4)\n",
    "\n",
    "(acc5, mean5, standard5) = n_folds(multi_nb_experiment, movie_dev, folds=9, ngram_range=[1,2], binary=False)\n",
    "if max(acc5) > highest:\n",
    "    highest = max(acc5)\n",
    "\n",
    "(acc6, mean6, standard6) = n_folds(multi_nb_experiment, movie_dev, folds=9, ngram_range=[1,3], binary=False)\n",
    "if max(acc6) > highest:\n",
    "    highest = max(acc6)\n",
    "\n",
    "print(\"highest:\", highest)\n",
    "# It seems that boolean=True and ngram_range=[1,3] yields the best accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(CountVectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: [0.855, 0.83, 0.87, 0.85, 0.885, 0.86, 0.865, 0.89, 0.845]\n",
      "mean: 0.8611111111111112\n",
      "Standard: 0.019002923751652315\n"
     ]
    }
   ],
   "source": [
    "def multi_LR_experiment(train_data, test_data, ngram_range, binary):\n",
    "    def separate(data):\n",
    "        texts = []\n",
    "        targets = []\n",
    "        for review in data:\n",
    "            text, sentiment = review\n",
    "            texts.append(text)\n",
    "            targets.append(sentiment)\n",
    "        return (texts, targets)\n",
    "    train_texts, train_target = separate(train_data)\n",
    "    test_texts, test_target = separate(test_data)\n",
    "    \n",
    "    v = CountVectorizer(ngram_range=ngram_range, binary=binary)\n",
    "    v.fit(train_texts)\n",
    "    \n",
    "    train_vectors = v.transform(train_texts)\n",
    "    test_vectors = v.transform(test_texts)\n",
    "    \n",
    "    \n",
    "    clf = LogisticRegression(random_state=0, solver='lbfgs')\n",
    "    clf.fit(train_vectors, train_target)\n",
    "    \n",
    "    prediction = clf.predict(test_vectors)\n",
    "    accuracy = clf.score(test_vectors, test_target)\n",
    "    \n",
    "    return (accuracy, prediction)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "(acc, mean, standard) = n_folds(multi_LR_experiment, movie_dev, folds=9, ngram_range=[1,3], binary=True)\n",
    "print(\"Acc:\", acc)\n",
    "print(\"mean:\", mean)\n",
    "print(\"Standard:\", standard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [(more.words(fileid), category)\n",
    "             for category in more.categories()\n",
    "             for fileid in more.fileids(category)]\n",
    "\n",
    "random.seed(15)\n",
    "random.shuffle(documents)\n",
    "\n",
    "# Last verification\n",
    "doc_test = documents[:200]\n",
    "# Testing in dev\n",
    "doc_dev_test = documents[200:400]\n",
    "# Trainingset\n",
    "doc_train = documents[400:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bernoulli_exp_p(train_data, test_data, feature_numbs=2000):\n",
    "    all_words = nltk.FreqDist(w.lower() for w in more.words())\n",
    "    word_features = list(all_words)[:feature_numbs]\n",
    "                                \n",
    "    def document_features(document): \n",
    "        document_words = set(document) \n",
    "        features = {}\n",
    "        for word in word_features:\n",
    "            features['contains({})'.format(word)] = (word in document_words)\n",
    "        return features\n",
    "    \n",
    "    train_data = [(document_features(document), label) for (document, label) in train_data]\n",
    "    test_data  = [(document_features(document), label) for (document, label) in test_data]\n",
    "    \n",
    "    def seperate(data):\n",
    "        texts = []\n",
    "        targets = []\n",
    "        for t, f in data:\n",
    "            texts.append(t)\n",
    "            targets.append(f)\n",
    "        return texts, targets\n",
    "\n",
    "    vec = DictVectorizer()\n",
    "    \n",
    "    train, train_target = seperate(train_data)\n",
    "    test, test_target   = seperate(test_data)\n",
    "\n",
    "    vec.fit(train)\n",
    "    \n",
    "    train_vec = vec.transform(train)\n",
    "    test_vec  = vec.transform(test)\n",
    "\n",
    "    clf = BernoulliNB()\n",
    "    clf.fit(train_vec, train_target)\n",
    "    \n",
    "    pred = clf.predict(test_vec)\n",
    "    score = clf.score(test_vec, test_target)\n",
    "\n",
    "    return pred, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_fold_n(experiment, dev_data, folds=9, feature_numbs=2000):\n",
    "    \n",
    "    size  = len(dev_data)/folds\n",
    "    acc_sum = 0\n",
    "    sum_squared_diff = 0\n",
    "    accuracies = []\n",
    "    data_folds = []\n",
    "    \n",
    "    for x in range(folds):\n",
    "        data_folds.append(dev_data[(x*int(size)):((x+1)*int(size))])\n",
    "    \n",
    "    for run in range(folds):\n",
    "        train_tmp = [data_folds[x] for x in range(len(data_folds)) if x != run]\n",
    "        test = data_folds[run]\n",
    "        train = []\n",
    "        for x in train_tmp:\n",
    "            train = train + x\n",
    "        (pred, acc) = experiment(train, test, feature_numbs)\n",
    "        accuracies.append(acc)\n",
    "        acc_sum = acc_sum + acc\n",
    "        \n",
    "    mean = acc_sum/folds\n",
    "    \n",
    "    for x in accuracies:\n",
    "        sum_squared_diff = sum_squared_diff + ((x-mean)*(x-mean))\n",
    "    standard_deviation = math.sqrt(sum_squared_diff/(folds-1))\n",
    "    \n",
    "    return (accuracies, mean, standard_deviation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8061111111111111 0.03533569174519031\n"
     ]
    }
   ],
   "source": [
    "data = doc_train + doc_dev_test \n",
    "\n",
    "accuracies, mean, std = n_fold_n(bernoulli_exp_p, data)\n",
    "\n",
    "print(mean, std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logisticReg_exp(train_data, test_data, feature_numbs=2000):\n",
    "    all_words = nltk.FreqDist(w.lower() for w in more.words())\n",
    "    word_features = list(all_words)[:feature_numbs]\n",
    "                                \n",
    "    def document_features(document): \n",
    "        document_words = set(document) \n",
    "        features = {}\n",
    "        for word in word_features:\n",
    "            features['contains({})'.format(word)] = (word in document_words)\n",
    "        return features\n",
    "    \n",
    "    train_data = [(document_features(document), label) for (document, label) in train_data]\n",
    "    test_data  = [(document_features(document), label) for (document, label) in test_data]\n",
    "    \n",
    "    def seperate(data):\n",
    "        texts = []\n",
    "        targets = []\n",
    "        for t, f in data:\n",
    "            texts.append(t)\n",
    "            targets.append(f)\n",
    "        return texts, targets\n",
    "\n",
    "    vec = DictVectorizer()\n",
    "    \n",
    "    train, train_target = seperate(train_data)\n",
    "    test, test_target   = seperate(test_data)\n",
    "\n",
    "    vec.fit(train)\n",
    "    \n",
    "    train_vec = vec.transform(train)\n",
    "    test_vec  = vec.transform(test)\n",
    "\n",
    "    clf = LogisticRegression(random_state=0, solver='lbfgs', max_iter=200)\n",
    "    clf.fit(train_vec, train_target)\n",
    "    \n",
    "    pred = clf.predict(test_vec)\n",
    "    score = clf.score(test_vec, test_target)\n",
    "\n",
    "    return pred, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.765, 0.84, 0.795, 0.795, 0.83, 0.77, 0.835, 0.81, 0.79]\n",
      "-\n",
      "0.8033333333333332 0.027386127875258286\n"
     ]
    }
   ],
   "source": [
    "accuracies, mean, std = n_fold_n(logisticReg_exp, data, folds=9)\n",
    "print(accuracies)\n",
    "print(\"-\")\n",
    "print(mean, std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 1000 most common words, bernoulli:\n",
      "Accuracies, bernoulli:  [0.8, 0.765, 0.785, 0.765, 0.755, 0.77, 0.845, 0.755, 0.75]\n",
      "Mean, bernoulli:  0.7766666666666667\n",
      "Standard deviation, bernoulli:  0.030103986446980733\n",
      "-\n",
      "The 2000 most common words, bernoulli:\n",
      "Accuracies, bernoulli:  [0.775, 0.805, 0.77, 0.815, 0.83, 0.795, 0.885, 0.78, 0.8]\n",
      "Mean, bernoulli:  0.8061111111111111\n",
      "Standard deviation, bernoulli:  0.03533569174519031\n",
      "-\n",
      "The 5000 most common words, bernoulli:\n",
      "Accuracies, bernoulli:  [0.81, 0.805, 0.74, 0.8, 0.81, 0.815, 0.885, 0.805, 0.835]\n",
      "Mean, bernoulli:  0.8116666666666666\n",
      "Standard deviation, bernoulli:  0.03758324094593227\n",
      "-\n",
      "The 10000 most common words, bernoulli:\n",
      "Accuracies, bernoulli:  [0.82, 0.835, 0.77, 0.82, 0.83, 0.82, 0.845, 0.81, 0.84]\n",
      "Mean, bernoulli:  0.821111111111111\n",
      "Standard deviation, bernoulli:  0.022189211592823894\n",
      "-\n",
      "The 20000 most common words, bernoulli:\n",
      "Accuracies, bernoulli:  [0.815, 0.82, 0.735, 0.83, 0.785, 0.795, 0.84, 0.795, 0.835]\n",
      "Mean, bernoulli:  0.8055555555555556\n",
      "Standard deviation, bernoulli:  0.03273419890233724\n",
      "-\n",
      "------------------------\n",
      "The 1000 most common words, LogisticRegression:\n",
      "Accuracies, LogisticRegression:  [0.755, 0.725, 0.745, 0.805, 0.765, 0.725, 0.775, 0.74, 0.73]\n",
      "Mean, LogisticRegression:  0.7516666666666667\n",
      "Standard deviation, LogisticRegression:  0.026575364531836648\n",
      "The 2000 most common words, LogisticRegression:\n",
      "Accuracies, LogisticRegression:  [0.765, 0.84, 0.795, 0.795, 0.83, 0.77, 0.835, 0.81, 0.79]\n",
      "Mean, LogisticRegression:  0.8033333333333332\n",
      "Standard deviation, LogisticRegression:  0.027386127875258286\n",
      "The 5000 most common words, LogisticRegression:\n",
      "Accuracies, LogisticRegression:  [0.8, 0.845, 0.825, 0.85, 0.85, 0.825, 0.845, 0.82, 0.855]\n",
      "Mean, LogisticRegression:  0.8350000000000001\n",
      "Standard deviation, LogisticRegression:  0.018371173070873825\n",
      "The 10000 most common words, LogisticRegression:\n",
      "Accuracies, LogisticRegression:  [0.81, 0.89, 0.86, 0.865, 0.855, 0.845, 0.855, 0.865, 0.835]\n",
      "Mean, LogisticRegression:  0.8533333333333332\n",
      "Standard deviation, LogisticRegression:  0.022220486043288964\n",
      "The 20000 most common words, LogisticRegression:\n",
      "Accuracies, LogisticRegression:  [0.82, 0.885, 0.865, 0.87, 0.845, 0.845, 0.88, 0.87, 0.855]\n",
      "Mean, LogisticRegression:  0.8594444444444443\n",
      "Standard deviation, LogisticRegression:  0.020378365434395824\n"
     ]
    }
   ],
   "source": [
    "common_words = [1000, 2000, 5000, 10000, 20000]\n",
    "\n",
    "for word_size in common_words:\n",
    "    accuracies, mean, std = n_fold_n(bernoulli_exp_p, data, feature_numbs=word_size)\n",
    "    print(\"The {} most common words, bernoulli:\".format(word_size))\n",
    "    print(\"Accuracies, bernoulli: \", accuracies)\n",
    "    print(\"Mean, bernoulli: \", mean)\n",
    "    print(\"Standard deviation, bernoulli: \", std)\n",
    "    print(\"-\")\n",
    "\n",
    "print(\"------------------------\")\n",
    "    \n",
    "for word_size in common_words:\n",
    "    accuracies, mean, std = n_fold_n(logisticReg_exp, data, feature_numbs=word_size)\n",
    "    print(\"The {} most common words, LogisticRegression:\".format(word_size))\n",
    "    print(\"Accuracies, LogisticRegression: \", accuracies)\n",
    "    print(\"Mean, LogisticRegression: \", mean)\n",
    "    print(\"Standard deviation, LogisticRegression: \", std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracies for movie_test, LogisticRegression:  [0.5909090909090909, 0.6363636363636364, 0.6363636363636364, 0.5, 0.36363636363636365, 0.45454545454545453, 0.5, 0.45454545454545453, 0.5454545454545454]\n",
      "Mean for movie_test, LogisticRegression:  0.5202020202020202\n",
      "Standard deviation for movie_test, LogisticRegression:  0.09122420135448708\n",
      "Accuracies for movie_test, Bernoilli:  [0.4090909090909091, 0.5, 0.2727272727272727, 0.5, 0.36363636363636365, 0.45454545454545453, 0.5454545454545454, 0.5909090909090909, 0.5909090909090909]\n",
      "Mean for movie_test, Bernoilli:  0.4696969696969697\n",
      "Standard deviation for movie_test, Bernoilli:  0.10660035817780524\n"
     ]
    }
   ],
   "source": [
    "(acc, mean, standard) = n_fold_n(logisticReg_exp, movie_test, feature_numbs=word_size)\n",
    "print(\"Accuracies for movie_test, LogisticRegression: \", acc)\n",
    "print(\"Mean for movie_test, LogisticRegression: \", mean)\n",
    "print(\"Standard deviation for movie_test, LogisticRegression: \", standard)\n",
    "\n",
    "(acc, mean, standard) = n_fold_n(bernoulli_exp_p, movie_test, feature_numbs=word_size)\n",
    "print(\"Accuracies for movie_test, Bernoilli: \", acc)\n",
    "print(\"Mean for movie_test, Bernoilli: \", mean)\n",
    "print(\"Standard deviation for movie_test, Bernoilli: \", standard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.54\n",
      "F-score: 0.5392628205128205\n",
      "Precision: 0.5393483709273184\n",
      "Recall: 0.539285356821139\n",
      "Acc: 0.515\n",
      "F-score: 0.33993399339933994\n",
      "Precision: 0.2575\n",
      "Recall: 0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "\n",
    "def seperate(data):\n",
    "    text = []\n",
    "    target = []\n",
    "    for t, f in data:\n",
    "        text.append(t)\n",
    "        target.append(f)\n",
    "    return text, target\n",
    "_, test_target = seperate(movie_test)\n",
    "\n",
    "pred, acc = logisticReg_exp(movie_dev, movie_test, feature_numbs=20000)\n",
    "\n",
    "print(\"Acc:\", acc)\n",
    "print('F-score:', f1_score(test_target, pred, average=\"macro\"))\n",
    "print('Precision:', precision_score(test_target, pred, average=\"macro\"))\n",
    "print('Recall:', recall_score(test_target, pred, average=\"macro\"))\n",
    "\n",
    "pred, acc = bernoulli_exp_p(movie_test, movie_test, feature_numbs=10000)\n",
    "\n",
    "print(\"Acc:\", acc)\n",
    "print('F-score:', f1_score(test_target, pred, average=\"macro\"))\n",
    "print('Precision:', precision_score(test_target, pred, average=\"macro\"))\n",
    "print('Recall:', recall_score(test_target, pred, average=\"macro\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose to use LogisticRegression with feature_numbs=20000 and Bernoulli with feature_numbs=10000\n",
    "This yielded the best results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
